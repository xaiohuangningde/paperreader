import streamlit as st
import pandas as pd
from io import BytesIO
import os
import sys
import traceback
from dotenv import load_dotenv
from PIL import Image
import logging

# è®¾ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å¼•å…¥å·¥å…·æ¨¡å—
try:
    from utils.pdf_processor import PDFProcessor
    from utils.ai_extractor import AIExtractor
    from utils.structured_extractor import StructuredExtractor
    from utils.report_generator import WordReportGenerator
    from utils.image_cropper import ImageCropper
    logger.info("âœ… æˆåŠŸå¯¼å…¥æ‰€æœ‰è‡ªå®šä¹‰æ¨¡å—")
except ImportError as e:
    st.error(f"âŒ æ ¸å¿ƒæ¨¡å—å¯¼å…¥å¤±è´¥: {str(e)}")
    st.error(f"è¯·ç¡®ä¿ utils æ–‡ä»¶å¤¹å­˜åœ¨ä¸”åŒ…å«æ‰€æœ‰å¿…éœ€çš„æ¨¡å—æ–‡ä»¶")
    st.error(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    st.error(f"ç³»ç»Ÿè·¯å¾„: {sys.path}")
    st.stop()

load_dotenv()

st.set_page_config(layout="wide", page_title="DeepSpec Debug Mode", initial_sidebar_state="expanded")

# --- å…¨å±€çŠ¶æ€åˆå§‹åŒ– ---
if 'papers_data' not in st.session_state:
    st.session_state.papers_data = {}
    logger.info("åˆå§‹åŒ– papers_data")
if 'current_file' not in st.session_state:
    st.session_state.current_file = None
    logger.info("åˆå§‹åŒ– current_file")
if 'word_buffer' not in st.session_state:
    st.session_state.word_buffer = None
    logger.info("åˆå§‹åŒ– word_buffer")
if 'debug_logs' not in st.session_state:
    st.session_state.debug_logs = []
    logger.info("åˆå§‹åŒ– debug_logs")

def add_debug_log(message):
    """æ·»åŠ è°ƒè¯•æ—¥å¿—"""
    timestamp = pd.Timestamp.now().strftime("%H:%M:%S")
    log_entry = f"[{timestamp}] {message}"
    st.session_state.debug_logs.append(log_entry)
    logger.info(log_entry)

# --- CSS æ ·å¼å¾®è°ƒ ---
st.markdown("""
<style>
    .stButton>button {width: 100%; border-radius: 5px;}
    .dataframe {font-family: 'Arial', sans-serif; font-size: 12px;}
    div[data-testid="stExpander"] details summary {font-weight: bold; color: #1f77b4;}
    .debug-log {background-color: #f1f1f1; padding: 10px; border-radius: 5px; font-family: monospace;}
</style>
""", unsafe_allow_html=True)

# ================= ä¾§è¾¹æ ï¼šå·¥ä½œæµæ§åˆ¶ =================
with st.sidebar:
    st.title("ğŸ“š DeepSpec è°ƒè¯•ç‰ˆ")
    
    # è°ƒè¯•é€‰é¡¹
    with st.expander("ğŸ”§ è°ƒè¯•é€‰é¡¹", expanded=False):
        debug_mode = st.checkbox("å¯ç”¨è¯¦ç»†è°ƒè¯•æ—¥å¿—", value=True)
        if st.button("æ¸…ç©ºæ—¥å¿—"):
            st.session_state.debug_logs = []
            st.rerun()
        
        # æ˜¾ç¤ºå½“å‰çŠ¶æ€
        st.write("å½“å‰çŠ¶æ€:")
        st.json({
            "papers_data_count": len(st.session_state.papers_data),
            "current_file": st.session_state.current_file,
            "word_buffer": st.session_state.word_buffer is not None,
            "debug_logs_count": len(st.session_state.debug_logs)
        })
    
    st.divider()
    
    # 1. æ‰¹é‡ä¸Šä¼ 
    uploaded_files = st.file_uploader("1. æ‰¹é‡ä¸Šä¼  PDF", type="pdf", accept_multiple_files=True)
    if uploaded_files:
        for f in uploaded_files:
            if f.name not in st.session_state.papers_data:
                st.session_state.papers_data[f.name] = {
                    "file_obj": f,
                    "status": "å¾…åˆ†æ",
                    "extracted_data": None,
                    "pdf_processor": PDFProcessor(),
                    "selected_image": None,
                    "error_log": []
                }
                add_debug_log(f"æ·»åŠ æ–°æ–‡ä»¶: {f.name}")
    
    st.divider()
    
    # 2. AI è®¾ç½®ä¸æå–
    role = st.selectbox("è®¾å®š AI è§’è‰²", ["æ°´åŠ›å‹è£‚ä¸“å®¶", "æ²¹è—æ•°å€¼æ¨¡æ‹Ÿä¸“å®¶", "æœºå™¨å­¦ä¹ ä¸“å®¶"])
    api_key = st.text_input("OpenAI API Key", type="password")
    if api_key: 
        os.environ["OPENAI_API_KEY"] = api_key
        add_debug_log("API Key å·²è®¾ç½®")
    
    # æ¨¡æ‹Ÿæ•°æ®é€‰é¡¹
    use_mock_data = st.checkbox("ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ® (æ— éœ€API Key)", value=True)
    
    pending_files = [name for name, info in st.session_state.papers_data.items() if info['status'] == "å¾…åˆ†æ"]
    if pending_files:
        st.info(f"é˜Ÿåˆ—å¾…å¤„ç†: {len(pending_files)} ç¯‡")
        if st.button(f"ğŸš€ æ‰¹é‡ AI æå–", type="primary"):
            if not api_key and not use_mock_data:
                st.error("è¯·å…ˆé…ç½® API Key æˆ–å‹¾é€‰ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            else:
                progress_bar = st.progress(0)
                for idx, fname in enumerate(pending_files):
                    info = st.session_state.papers_data[fname]
                    add_debug_log(f"å¼€å§‹å¤„ç†: {fname}")
                    
                    try:
                        # é¢„å¤„ç† PDF
                        add_debug_log(f"å¤„ç† PDF: {fname}")
                        success = info['pdf_processor'].process_pdf(info['file_obj'])
                        if not success:
                            raise Exception("PDF å¤„ç†å¤±è´¥")
                        
                        # æå–æ–‡æœ¬
                        text = ""
                        for page_num in range(1, min(4, info['pdf_processor'].get_page_count() + 1)):
                            page_text = info['pdf_processor'].extract_text_by_page(page_num)
                            if page_text:
                                text += page_text + "\n\n"
                        
                        add_debug_log(f"æå–æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
                        
                        # AI æå–
                        if use_mock_data:
                            # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
                            extractor = StructuredExtractor()
                            data = extractor.get_mock_structured_data()
                            data['title'] = f"æ¨¡æ‹Ÿæ•°æ® - {fname}"
                        else:
                            extractor = StructuredExtractor()
                            data = extractor.extract_structured_data(text, role=role)
                        
                        info['extracted_data'] = data
                        info['status'] = "å·²æå–"
                        add_debug_log(f"æˆåŠŸæå–æ•°æ®: {fname}")
                        
                    except Exception as e:
                        error_msg = f"{fname} æå–å¤±è´¥: {str(e)}"
                        st.error(error_msg)
                        info['error_log'].append(error_msg)
                        add_debug_log(error_msg)
                        
                        # æ·»åŠ è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯åˆ°æ—¥å¿—
                        add_debug_log(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                        
                        # ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ä½œä¸ºåå¤‡
                        extractor = StructuredExtractor()
                        data = extractor.get_mock_structured_data()
                        data['title'] = f"åå¤‡æ•°æ® - {fname} (æå–å¤±è´¥)"
                        info['extracted_data'] = data
                        info['status'] = "å·²æå–(åå¤‡)"
                        add_debug_log(f"ä½¿ç”¨åå¤‡æ•°æ®: {fname}")
                    
                    progress_bar.progress((idx + 1) / len(pending_files))
                
                st.success("æå–å®Œæˆï¼è¯·åœ¨å³ä¾§é€ä¸€å®¡æ ¸ã€‚")
                st.rerun()
    
    st.divider()

    # 3. è®ºæ–‡å¯¼èˆª
    st.subheader("ğŸ“‘ è®ºæ–‡åˆ—è¡¨")
    if st.session_state.papers_data:
        for fname, info in st.session_state.papers_data.items():
            icon = "âœ…" if info['status'] == "å·²å®¡æ ¸" else ("ğŸ¤–" if "å·²æå–" in info['status'] else "â³")
            status_color = "green" if info['status'] == "å·²å®¡æ ¸' else ("orange" if "å·²æå–" in info['status'] else "gray")
            with st.container():
                st.markdown(f"<span style='color:{status_color}'>{icon}</span> **{fname}** - {info['status']}", unsafe_allow_html=True)
                if st.button(f"ç¼–è¾‘", key=f"nav_{fname}"):
                    st.session_state.current_file = fname
                if info['error_log']:
                    with st.expander(f"é”™è¯¯æ—¥å¿— ({len(info['error_log'])})"):
                        for error in info['error_log']:
                            st.error(error)
    else:
        st.info("æš‚æ— ä¸Šä¼ çš„æ–‡ä»¶")

# ================= ä¸»å·¥ä½œåŒº =================

# åˆ›å»ºä¸¤ä¸ª Tabï¼šä¸€ä¸ªæ˜¯å•ç¯‡ç¼–è¾‘ï¼Œä¸€ä¸ªæ˜¯å…¨å±€é¢„è§ˆ
tab_edit, tab_preview, tab_debug = st.tabs(["âœï¸ å•ç¯‡ç²¾ä¿® (Editor)", "ğŸ‘€ æŠ¥å‘Šé¢„è§ˆ (Word Preview)", "ğŸ” è°ƒè¯•æ—¥å¿—"])

# --- Tab 1: å•ç¯‡ç²¾ä¿® ---
with tab_edit:
    if st.session_state.current_file:
        fname = st.session_state.current_file
        info = st.session_state.papers_data[fname]
        
        st.caption(f"å½“å‰æ­£åœ¨ç¼–è¾‘: {fname} | çŠ¶æ€: {info['status']}")
        
        if "å¾…åˆ†æ" in info['status']:
            st.warning("âš ï¸ æ­¤æ–‡ä»¶å°šæœªè¿›è¡Œ AI æå–ï¼Œè¯·å…ˆåœ¨å·¦ä¾§ç‚¹å‡»"æ‰¹é‡ AI æå–"ã€‚")
        else:
            if info['extracted_data']:
                data = info['extracted_data']
                
                # æ˜¾ç¤ºåŸå§‹æ•°æ®ç”¨äºè°ƒè¯•
                with st.expander("ğŸ” åŸå§‹æå–æ•°æ® (è°ƒè¯•)", expanded=False):
                    st.json(data)
                
                # åŒæ å¸ƒå±€ï¼šå·¦ç¼–è¾‘ï¼Œå³å›¾è¡¨
                col_form, col_media = st.columns([1.3, 1])
                
                with col_form:
                    st.subheader("1. ç»“æ„åŒ–æ•°æ®æ ¡å¯¹")
                    with st.container(border=True):
                        new_title = st.text_input("è®ºæ–‡æ ‡é¢˜ (Article)", data.get('title', ''))
                        new_purpose = st.text_area("ç ”ç©¶ç›®çš„ (Purpose)", data.get('purpose', ''), height=80)
                        
                        # ç»“è®ºç¼–è¾‘
                        st.markdown("**æ ¸å¿ƒç»“è®º (Conclusions)**")
                        conclusions = data.get('conclusion', [])
                        if isinstance(conclusions, str): 
                            conclusions = [conclusions]
                        if not conclusions:
                            conclusions = [""]  # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªç©ºç»“è®º
                        
                        new_conclusions = []
                        for i, c in enumerate(conclusions):
                            new_c = st.text_area(f"ç»“è®º {i+1}", c, key=f"c_{fname}_{i}", height=60)
                            new_conclusions.append(new_c)
                        
                        # æ·»åŠ æ–°ç»“è®ºæŒ‰é’®
                        if st.button(f"+ æ·»åŠ ç»“è®º", key=f"add_conclusion_{fname}"):
                            new_conclusions.append("")
                            st.rerun()
                        
                        new_params = st.text_area("å…³é”®å‚æ•° (Parameters)", data.get('params', ''), height=100)
                        
                        # å…¬å¼ç¼–è¾‘
                        st.markdown("**æ§åˆ¶æ–¹ç¨‹ (Formulas)**")
                        formulas = data.get('formulas', [])
                        if isinstance(formulas, str): 
                            formulas = [formulas]
                        if not formulas:
                            formulas = [""]  # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªç©ºå…¬å¼
                        
                        new_formulas = []
                        for i, f in enumerate(formulas):
                            f_col1, f_col2 = st.columns([3, 1])
                            with f_col1:
                                new_f = st.text_input(f"LaTeX Code {i+1}", f, key=f"f_{fname}_{i}")
                            with f_col2:
                                try:
                                    if new_f.strip():
                                        st.latex(new_f)
                                except Exception as e:
                                    st.caption(f"æ¸²æŸ“å¤±è´¥: {str(e)[:20]}...")
                            new_formulas.append(new_f)
                        
                        # æ·»åŠ æ–°å…¬å¼æŒ‰é’®
                        if st.button(f"+ æ·»åŠ å…¬å¼", key=f"add_formula_{fname}"):
                            new_formulas.append("")
                            st.rerun()
                        
                        new_comments = st.text_area("ä¸“å®¶æ‰¹æ³¨ (Comments)", data.get('comments', ''))
                        new_why = st.text_input("æ ‡ç­¾ (Why)", data.get('why', ''))

                with col_media:
                    st.subheader("2. å›¾è¡¨è¯æ®é“¾ (Evidence)")
                    
                    if not info['pdf_processor'].pages:
                        try:
                            success = info['pdf_processor'].process_pdf(info['file_obj'])
                            if not success:
                                st.error("PDF å¤„ç†å¤±è´¥")
                        except Exception as e:
                            st.error(f"PDF å¤„ç†å‡ºé”™: {str(e)}")
                    
                    total_pages = info['pdf_processor'].get_page_count()
                    if total_pages > 0:
                        page_num = st.number_input("é€‰æ‹© PDF é¡µç ", 1, total_pages, 1, key=f"pg_{fname}")
                        
                        try:
                            page_img = ImageCropper.extract_pdf_page_as_image(info['pdf_processor'], page_num)
                            if page_img:
                                st.info("ğŸ‘‡ åœ¨ä¸‹æ–¹æ‹–æ‹½æ¡†é€‰å…³é”®å›¾è¡¨ï¼Œç„¶åç‚¹å‡»"æˆªå–"")
                                cropped = ImageCropper.crop_image_with_streamlit(page_img, key_prefix=f"crop_{fname}")
                                
                                if st.button("ğŸ“¸ ç¡®è®¤æˆªå–å¹¶ä½¿ç”¨", key=f"btn_crop_{fname}"):
                                    info['selected_image'] = cropped
                                    st.success("æˆªå›¾å·²ç¼“å­˜ï¼")
                                
                                if info['selected_image']:
                                    st.image(info['selected_image'], caption="å½“å‰å·²ç»‘å®šçš„å›¾è¡¨", width=200)
                                else:
                                    st.warning("å°šæœªç»‘å®šå›¾è¡¨")
                            else:
                                st.error(f"æ— æ³•æå–ç¬¬ {page_num} é¡µå›¾åƒ")
                        except Exception as e:
                            st.error(f"å›¾åƒå¤„ç†å‡ºé”™: {str(e)}")
                    else:
                        st.error("PDF æ— å¯ç”¨é¡µé¢")

                st.divider()
                if st.button("ğŸ’¾ ä¿å­˜å¹¶æ ‡è®°ä¸º[å·²å®¡æ ¸]", type="primary", key=f"save_{fname}"):
                    try:
                        info['extracted_data'] = {
                            'title': new_title, 
                            'purpose': new_purpose,
                            'conclusion': new_conclusions, 
                            'params': new_params,
                            'formulas': new_formulas, 
                            'comments': new_comments, 
                            'why': new_why
                        }
                        info['status'] = "å·²å®¡æ ¸"
                        st.session_state.word_buffer = None  # æ•°æ®å˜æ›´ï¼Œæ¸…é™¤æ—§ç¼“å­˜
                        st.toast("ä¿å­˜æˆåŠŸï¼è¯·ç»§ç»­ä¸‹ä¸€ç¯‡æˆ–å»é¢„è§ˆé¡µæŸ¥çœ‹ã€‚")
                        add_debug_log(f"ä¿å­˜æˆåŠŸ: {fname}")
                        st.rerun()
                    except Exception as e:
                        st.error(f"ä¿å­˜å¤±è´¥: {str(e)}")
                        add_debug_log(f"ä¿å­˜å¤±è´¥: {fname} - {str(e)}")
            else:
                st.error("æå–æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç¼–è¾‘")
                add_debug_log(f"æå–æ•°æ®ä¸ºç©º: {fname}")
    else:
        st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§é€‰æ‹©ä¸€ç¯‡è®ºæ–‡è¿›è¡Œç¼–è¾‘ã€‚")

# --- Tab 2: æŠ¥å‘Šé¢„è§ˆ ---
with tab_preview:
    st.subheader("ğŸ“„ æœ€ç»ˆæŠ¥å‘Šé¢„è§ˆ (Master Table View)")
    
    reviewed_papers = [p for p in st.session_state.papers_data.values() if p['status'] == "å·²å®¡æ ¸"]
    
    if not reviewed_papers:
        st.warning("âš ï¸ æš‚æ— å·²å®¡æ ¸çš„è®ºæ–‡ã€‚è¯·åœ¨"å•ç¯‡ç²¾ä¿®"é¡µé¢å®Œæˆå®¡æ ¸å¹¶ç‚¹å‡»ä¿å­˜ã€‚")
    else:
        st.write(f"å…± {len(reviewed_papers)} ç¯‡è®ºæ–‡å‡†å¤‡ç”Ÿæˆã€‚")
        
        # 1. å‡†å¤‡é¢„è§ˆæ•°æ® (Pandas DataFrame)
        preview_list = []
        for p in reviewed_papers:
            if p['extracted_data']:
                d = p['extracted_data']
                # æ ¼å¼åŒ–ç»“è®ºä¸ºå­—ç¬¦ä¸²
                cons_str = "\n".join([f"{i+1}. {c}" for i, c in enumerate(d.get('conclusion', []))])
                # æ ¼å¼åŒ–å…¬å¼
                forms_str = "\n".join(d.get('formulas', []))
                
                preview_list.append({
                    "Article": d.get('title'),
                    "å…·ä½“å†…å®¹(1): ç›®çš„ä¸ç»“è®º": f"ã€ç›®çš„ã€‘\n{d.get('purpose')}\n\nã€ç»“è®ºã€‘\n{cons_str}",
                    "å…·ä½“å†…å®¹(2): å‚æ•°/å…¬å¼/å›¾è¡¨": f"ã€å‚æ•°ã€‘\n{d.get('params')}\n\nã€å…¬å¼ã€‘\n{forms_str}\n\nã€å›¾è¡¨ã€‘\n{'âœ… å·²åŒ…å«å›¾ç‰‡' if p['selected_image'] else 'âŒ æ— å›¾ç‰‡'}",
                    "Comments": d.get('comments'),
                    "Why": d.get('why')
                })
            else:
                add_debug_log("æŸç¯‡è®ºæ–‡çš„æå–æ•°æ®ä¸ºç©ºï¼Œæ— æ³•é¢„è§ˆ")
        
        if preview_list:
            df_preview = pd.DataFrame(preview_list)
            st.dataframe(df_preview, use_container_width=True)
        else:
            st.error("æ²¡æœ‰å¯ç”¨äºé¢„è§ˆçš„æ•°æ®")
        
        st.divider()
        
        # 2. ç”Ÿæˆä¸ä¸‹è½½åŒºåŸŸ
        col_gen, col_down = st.columns([1, 2])
        
        with col_gen:
            # å¼ºåˆ¶é‡æ–°ç”ŸæˆæŒ‰é’®
            if st.button("ğŸ”„ ç”Ÿæˆ/æ›´æ–° Word æ–‡ä»¶"):
                try:
                    with st.spinner("æ­£åœ¨æ’ç‰ˆ Word æ–‡æ¡£ (åŒ…å«é«˜æ¸…å›¾ç‰‡ä¸å…¬å¼æ¸²æŸ“)..."):
                        gen = WordReportGenerator()
                        for p in reviewed_papers:
                            img_stream = None
                            if p['selected_image']:
                                img_stream = BytesIO()
                                p['selected_image'].save(img_stream, format='PNG')
                                img_stream.seek(0)
                            try:
                                gen.add_paper_analysis(p['extracted_data'], img_stream)
                                add_debug_log(f"æˆåŠŸæ·»åŠ è®ºæ–‡åˆ°Word: {p['extracted_data'].get('title', 'æœªçŸ¥')}")
                            except Exception as e:
                                add_debug_log(f"æ·»åŠ è®ºæ–‡åˆ°Wordå¤±è´¥: {str(e)}")
                        
                        st.session_state.word_buffer = gen.save_to_bytes()
                    st.success("ç”Ÿæˆå®Œæ¯•ï¼")
                except Exception as e:
                    st.error(f"ç”ŸæˆWordæ–‡æ¡£å¤±è´¥: {str(e)}")
                    add_debug_log(f"Wordç”Ÿæˆå¤±è´¥: {str(e)}\n{traceback.format_exc()}")
        
        with col_down:
            if st.session_state.word_buffer:
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½æœ€ç»ˆ Word æŠ¥å‘Š (.docx)",
                    data=st.session_state.word_buffer,
                    file_name="SPE_Literature_Review_Master.docx",
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                    type="primary"
                )
            else:
                st.caption("ç‚¹å‡»å·¦ä¾§æŒ‰é’®ç”Ÿæˆåå³å¯ä¸‹è½½")

# --- Tab 3: è°ƒè¯•æ—¥å¿— ---
with tab_debug:
    st.subheader("ğŸ” è°ƒè¯•æ—¥å¿—")
    
    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
    with st.expander("ç³»ç»Ÿä¿¡æ¯", expanded=False):
        st.write(f"Python ç‰ˆæœ¬: {sys.version}")
        st.write(f"Streamlit ç‰ˆæœ¬: {st.__version__}")
        st.write(f"å·¥ä½œç›®å½•: {os.getcwd()}")
        st.write(f"æ–‡ä»¶ç³»ç»Ÿåˆ—è¡¨: {os.listdir('.')}")
        
        # æ£€æŸ¥utilsç›®å½•
        utils_dir = "utils"
        if os.path.exists(utils_dir):
            st.write(f"Utils ç›®å½•å­˜åœ¨: {os.listdir(utils_dir)}")
        else:
            st.error(f"Utils ç›®å½•ä¸å­˜åœ¨: {utils_dir}")
    
    # æ˜¾ç¤ºè°ƒè¯•æ—¥å¿—
    if st.session_state.debug_logs:
        st.write("### è°ƒè¯•æ—¥å¿—")
        for log in st.session_state.debug_logs[-50:]:  # åªæ˜¾ç¤ºæœ€è¿‘50æ¡
            st.code(log, language="text")
    else:
        st.info("æš‚æ— è°ƒè¯•æ—¥å¿—")